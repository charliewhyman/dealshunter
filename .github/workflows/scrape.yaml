name: Shopify Scraper
on:
  schedule:
    # Daily at 2am UTC - scrape only products
    - cron: '0 2 * * *'
    # Monthly on the 1st at 3am UTC - shop refresh + maintenance
    - cron: '0 3 1 * *'
  workflow_dispatch:
    inputs:
      mode:
        description: 'Scraping mode'
        required: false
        default: 'all'
        type: choice
        options:
          - all
          - scrape
          - upload
      skip_shops:
        description: 'Skip shop data scraping'
        required: false
        default: false
        type: boolean

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: ${{ github.event.schedule == '0 3 1 * *' && 420 || 180 }}
    environment: production

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v3

      - name: Set up Python
        run: uv python install 3.11

      - name: Cache uv dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/uv
            .venv
          key: ${{ runner.os }}-uv-${{ hashFiles('scraping/uv.lock', 'scraping/pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-uv-

      - name: Install dependencies
        run: |
          cd scraping
          uv sync --frozen

      - name: Determine scraping strategy
        id: strategy
        run: |
          if [ "${{ github.event.schedule }}" == "0 2 * * *" ]; then
            echo "Running DAILY PRODUCT SCRAPE"
            echo "skip_shops=true" >> $GITHUB_OUTPUT
            echo "shop_update_days=0" >> $GITHUB_OUTPUT
            echo "max_concurrent=8" >> $GITHUB_OUTPUT
            echo "batch_size=15" >> $GITHUB_OUTPUT
            echo "limit_shops=0" >> $GITHUB_OUTPUT
            echo "scrape_type=daily" >> $GITHUB_OUTPUT
            echo "full_product_scrape=false" >> $GITHUB_OUTPUT
            echo "timeout_minutes=180" >> $GITHUB_OUTPUT
          elif [ "${{ github.event.schedule }}" == "0 3 1 * *" ]; then
            echo "Running MONTHLY SHOP + MAINTENANCE"
            echo "skip_shops=false" >> $GITHUB_OUTPUT
            echo "shop_update_days=30" >> $GITHUB_OUTPUT
            echo "max_concurrent=4" >> $GITHUB_OUTPUT
            echo "batch_size=6" >> $GITHUB_OUTPUT
            echo "limit_shops=0" >> $GITHUB_OUTPUT
            echo "scrape_type=monthly" >> $GITHUB_OUTPUT
            echo "full_product_scrape=false" >> $GITHUB_OUTPUT
            echo "timeout_minutes=420" >> $GITHUB_OUTPUT
          else
            echo "Running MANUAL SCRAPE with custom parameters"
            echo "skip_shops=${{ github.event.inputs.skip_shops || 'false' }}" >> $GITHUB_OUTPUT
            echo "scrape_type=manual" >> $GITHUB_OUTPUT
          fi

      - name: Run scraper
        id: run_scraper
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SECRET_KEY: ${{ secrets.SUPABASE_SECRET_KEY }}
          ENVIRONMENT: production
        run: |
          cd scraping
          CMD="uv run python run.py --mode=all"
          CMD="$CMD --max-concurrent=${{ steps.strategy.outputs.max_concurrent }}"
          CMD="$CMD --batch-size=${{ steps.strategy.outputs.batch_size }}"
          if [ "${{ steps.strategy.outputs.skip_shops }}" == "true" ]; then
            CMD="$CMD --skip-shops"
          fi
          if [ "${{ steps.strategy.outputs.shop_update_days }}" != "0" ]; then
            CMD="$CMD --shop-update-days=${{ steps.strategy.outputs.shop_update_days }}"
          fi
          timeout $(( ${{ steps.strategy.outputs.timeout_minutes }} * 60 )) $CMD
          EXIT_CODE=$?
          if [ $EXIT_CODE -ne 0 ]; then
            echo "ERROR: Scraper failed with exit code $EXIT_CODE"
            exit $EXIT_CODE
          fi

      - name: Run maintenance (monthly only)
        if: steps.strategy.outputs.scrape_type == 'monthly'
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SECRET_KEY: ${{ secrets.SUPABASE_SECRET_KEY }}
        run: |
          cd scraping
          echo "Running database maintenance..."
          uv run python run.py --mode=db --maintenance

      - name: Upload artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scrape-artifacts-${{ github.run_number }}
          path: |
            scraping/data/
            scraping/logs/
          retention-days: 7

name: Shopify Scraper
on:
  schedule:
    # Daily at 2am UTC - skip shop data scraping, faster concurrency
    - cron: '0 2 * * *'
    # Weekly on Sunday at 3am UTC - full scrape including shops, slower concurrency
    - cron: '0 3 * * 0'
  workflow_dispatch:
    inputs:
      mode:
        description: 'Scraping mode'
        required: false
        default: 'all'
        type: choice
        options:
          - all
          - scrape
          - upload
      skip_shops:
        description: 'Skip shop data scraping'
        required: false
        default: false
        type: boolean
      shop_update_days:
        description: 'Only scrape shops older than N days (0 = scrape all)'
        required: false
        default: '0'
        type: string
      shop_id:
        description: 'Specific shop ID(s) to scrape (comma-separated)'
        required: false
        default: ''
        type: string
      max_concurrent:
        description: 'Maximum concurrent shops to scrape (default: 3)'
        required: false
        default: '3'
        type: string
      batch_size:
        description: 'Batch size for processing shops (default: 5)'
        required: false
        default: '5'
        type: string
      limit_shops:
        description: 'Limit total number of shops to scrape (0 = no limit)'
        required: false
        default: '0'
        type: string
      timeout_minutes:
        description: 'Timeout in minutes (default: 180)'
        required: false
        default: '180'
        type: string

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: ${{ github.event.schedule == '0 3 * * 0' && 300 || 180 }}
    environment: production
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Install uv
        uses: astral-sh/setup-uv@v3
        
      - name: Set up Python
        run: uv python install 3.11
        
      - name: Cache uv dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/uv
            .venv
          key: ${{ runner.os }}-uv-${{ hashFiles('scraping/uv.lock', 'scraping/pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-uv-
            
      - name: Install dependencies
        run: |
          cd scraping
          uv sync --frozen
      
      - name: Determine scraping strategy
        id: strategy
        run: |
          # Determine if this is the weekly full scrape (Sunday 3am)
          if [ "${{ github.event.schedule }}" == "0 3 * * 0" ]; then
            echo "Running WEEKLY FULL SCRAPE (including shop data)"
            echo "skip_shops=false" >> $GITHUB_OUTPUT
            echo "shop_update_days=7" >> $GITHUB_OUTPUT
            echo "max_concurrent=" >> $GITHUB_OUTPUT
            echo "batch_size=8" >> $GITHUB_OUTPUT
            echo "limit_shops=0" >> $GITHUB_OUTPUT
            echo "scrape_type=weekly_full" >> $GITHUB_OUTPUT
            echo "timeout_minutes=240" >> $GITHUB_OUTPUT  # ADDED: Specific timeout for weekly
          elif [ "${{ github.event.schedule }}" == "0 2 * * *" ]; then
            echo "Running DAILY SCRAPE (skip shop data)"
            echo "skip_shops=true" >> $GITHUB_OUTPUT
            echo "shop_update_days=30" >> $GITHUB_OUTPUT
            echo "max_concurrent=4" >> $GITHUB_OUTPUT
            echo "batch_size=10" >> $GITHUB_OUTPUT
            echo "limit_shops=100" >> $GITHUB_OUTPUT
            echo "scrape_type=daily" >> $GITHUB_OUTPUT
            echo "timeout_minutes=180" >> $GITHUB_OUTPUT  # ADDED: Specific timeout for daily
          else
            echo "Running MANUAL SCRAPE with custom parameters"
            echo "skip_shops=${{ inputs.skip_shops }}" >> $GITHUB_OUTPUT
            echo "shop_update_days=${{ inputs.shop_update_days }}" >> $GITHUB_OUTPUT
            echo "max_concurrent=${{ inputs.max_concurrent || 3 }}" >> $GITHUB_OUTPUT
            echo "batch_size=${{ inputs.batch_size || 5 }}" >> $GITHUB_OUTPUT
            echo "limit_shops=${{ inputs.limit_shops || 0 }}" >> $GITHUB_OUTPUT
            echo "scrape_type=manual" >> $GITHUB_OUTPUT
            echo "timeout_minutes=${{ inputs.timeout_minutes || 180 }}" >> $GITHUB_OUTPUT  # ADDED
          fi
      
      - name: Run scraper
        id: run_scraper
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
          SUPABASE_PUBLISHABLE_KEY: ${{ secrets.SUPABASE_PUBLISHABLE_KEY }}
          ENVIRONMENT: production
        run: |
          cd scraping
          
          # Verify environment variables
          if [ -z "$SUPABASE_URL" ]; then
            echo "ERROR: SUPABASE_URL is not set!"
            exit 1
          fi
          
          # Build command
          CMD="uv run python run.py --mode=${{ inputs.mode || 'all' }}"
          CMD="$CMD --max-concurrent=${{ steps.strategy.outputs.max_concurrent }}"
          CMD="$CMD --batch-size=${{ steps.strategy.outputs.batch_size }}"
          
          if [ "${{ steps.strategy.outputs.skip_shops }}" == "true" ]; then
            CMD="$CMD --skip-shops"
            echo "Mode: Daily scrape (shop data skipped)"
          elif [ "${{ steps.strategy.outputs.shop_update_days }}" != "0" ]; then
            CMD="$CMD --shop-update-days=${{ steps.strategy.outputs.shop_update_days }}"
            echo "Mode: Scraping shops older than ${{ steps.strategy.outputs.shop_update_days }} days"
          else
            echo "Mode: Full scrape (including all shop data)"
          fi
          
          if [ -n "${{ inputs.shop_id }}" ]; then
            CMD="$CMD --shop-id=${{ inputs.shop_id }}"
            echo "Filtering to shop(s): ${{ inputs.shop_id }}"
          fi
          
          echo "Configuration:"
          echo "  Max Concurrent: ${{ steps.strategy.outputs.max_concurrent }}"
          echo "  Batch Size: ${{ steps.strategy.outputs.batch_size }}"
          echo "  Skip Shops: ${{ steps.strategy.outputs.skip_shops }}"
          echo "  Shop Update Days: ${{ steps.strategy.outputs.shop_update_days }}"
          echo "  Timeout Minutes: ${{ steps.strategy.outputs.timeout_minutes || 180 }}"
          echo ""
          echo "Running: $CMD"
          
          # FIX: Increase buffer time to 10 minutes before timeout
          TIMEOUT_SECONDS=$(( (${{ steps.strategy.outputs.timeout_minutes || 180 }} - 10) * 60 ))
          echo "Setting timeout to $TIMEOUT_SECONDS seconds (${{ steps.strategy.outputs.timeout_minutes || 180 }} minutes - 10 minutes buffer)"
          
          timeout $TIMEOUT_SECONDS $CMD
          
          EXIT_CODE=$?
          echo "exit_code=$EXIT_CODE" >> $GITHUB_OUTPUT
          
          if [ $EXIT_CODE -eq 124 ]; then
            echo "WARNING: Scraper timed out after $TIMEOUT_SECONDS seconds"
            echo "Try increasing the timeout_minutes parameter"
            echo "Weekly scrapes may need 240+ minutes, daily scrapes 180+ minutes"
            
            # FIX: Don't fail the workflow on timeout, just warn
            echo "Continuing workflow despite timeout..."
          elif [ $EXIT_CODE -ne 0 ]; then
            echo "ERROR: Scraper failed with exit code $EXIT_CODE"
            exit $EXIT_CODE
          else
            echo "SUCCESS: Scraper completed within timeout"
          fi
          
      - name: Run database refresh (core only)
        if: success() && steps.strategy.outputs.scrape_type == 'weekly_full'
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
        run: |
          cd scraping
          echo "Running core database refresh..."
          uv run python run.py --mode=db --refresh-core
          
      - name: Upload artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scrape-artifacts-${{ github.run_number }}
          path: |
            scraping/data/
            scraping/logs/
          retention-days: 7